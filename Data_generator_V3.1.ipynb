{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_generator_V3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IceOnDunes/speech-recognition/blob/main/Data_generator_V3.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTX_otxg2Pyy"
      },
      "source": [
        "\"\"\" Classe Data generator pour génerer les données par batch durant l'apprentissage \"on the fly\" \n",
        "    code de classe inspiré de : https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
        "    Ajout de la partie pre-traitement (normalisation, calcul de spectrogramme, padding ...) \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from CTCModel import CTCModel as CTCModel\n",
        "from tensorflow.keras.optimizers  import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense, Input,BatchNormalization, Dropout, Activation, TimeDistributed, Activation, Bidirectional, LSTM\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import string\n",
        "import librosa\n",
        "from scipy.io import wavfile\n",
        "\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs,shuffle=False, batch_size=32, window_len=128,nfft=256,hop_len=127):\n",
        "        'Initialization'\n",
        "        self.list_IDs = list_IDs\n",
        "        self.hop_len = hop_len\n",
        "        self.batch_size = batch_size\n",
        "        self.nfft= nfft\n",
        "        self.window_len = window_len\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.char_dict =  { ' ': 0,\n",
        "                            'a': 1,\n",
        "                            'b' : 2,\n",
        "                            'c' : 3,\n",
        "                            'd' : 4,\n",
        "                            'e' : 5,\n",
        "                            'f' : 6,\n",
        "                            'g' : 7,\n",
        "                            'h' : 8,\n",
        "                            'i' : 9,\n",
        "                            'j' : 10,\n",
        "                            'k' : 11,\n",
        "                            'l' : 12,\n",
        "                            'm' : 13,\n",
        "                            'n' : 14,\n",
        "                            'o' : 15,\n",
        "                            'p' : 16,\n",
        "                            'q' : 17,\n",
        "                            'r' : 18,\n",
        "                            's' : 19,\n",
        "                            't' : 20,\n",
        "                            'u' : 21,\n",
        "                            'v' : 22,\n",
        "                            'w' : 23,\n",
        "                            'x' : 24,\n",
        "                            'y' : 25,\n",
        "                            'z' : 26\n",
        "                            }\n",
        "                          \n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def get_data (self,data_IDs):\n",
        "      \"\"\"renvoie le son ainsi que son text associé normalisé \"\"\"\n",
        "\n",
        "      son = []\n",
        "      text = []\n",
        "\n",
        "      \"récuperer les characters de ponctuations\"\n",
        "      ponctuation_char = set(string.punctuation)\n",
        "\n",
        "      for d in data_IDs:\n",
        "          fe, audio = wavfile.read(d)\n",
        "          moyenne = audio.mean()\n",
        "          std = audio.std()\n",
        "          audio = (audio-moyenne) / std\n",
        "          son.append(audio)\n",
        "\n",
        "          text_path = d[:-8] + '.TXT'  # conversion chemin audio vers chemin text correspondant\n",
        "          txt = open( text_path, 'r')\n",
        "          ligne = txt.readline()\n",
        "\n",
        "\n",
        "          # normaliser les texts\n",
        "          result = ''.join([i for i in ligne if (not i.isdigit() and i not in ponctuation_char )]).lstrip().lower().replace(\"\\n\", '')\n",
        "          text.append(result)\n",
        "      return son, text\n",
        "\n",
        "\n",
        "    def get_padded_mel_spectro(self,audio, maxlen):\n",
        "        mel_spectro =np.log(np.absolute(librosa.stft(audio.astype(float),  n_fft= self.nfft,hop_length=self.hop_len, win_length=self.window_len)))\n",
        "        x_len = mel_spectro.shape[1]\n",
        "\n",
        "        padded_spectro = tf.keras.preprocessing.sequence.pad_sequences(mel_spectro,maxlen=maxlen, dtype='float', padding='post', truncating='post',value=float(255))\n",
        "        return padded_spectro, x_len\n",
        "\n",
        "    def extract_features(self, x_data_init):\n",
        "      #extract longest spectrogram length\n",
        "        mel_spectrogram = np.log(np.absolute(librosa.stft(max(x_data_init, key=len).astype(float),n_fft= self.nfft,hop_length=self.hop_len, win_length=self.window_len)))\n",
        "        x_max_length = mel_spectrogram.shape[1]\n",
        "        # x_max_length = 640\n",
        "        x_data = []\n",
        "        x_data_len = []\n",
        "\n",
        "        for i in range(len(x_data_init)):\n",
        "\n",
        "            padded_spectro, x_len = self.get_padded_mel_spectro(x_data_init[i], maxlen=x_max_length)\n",
        "\n",
        "            x_data.append(padded_spectro.T)\n",
        "            x_data_len.append(x_len) \n",
        "\n",
        "        #convert to array\n",
        "        data_input = np.array(x_data)\n",
        "        input_length = np.array(x_data_len)\n",
        "\n",
        "        return data_input,input_length\n",
        "\n",
        "#changer les noms\n",
        "    def encode_text(self, y_data_init):\n",
        "\n",
        "        y_max_length = len(max(y_data_init, key=len))\n",
        "        y_data = []\n",
        "        y_data_len = []\n",
        "\n",
        "        for i in range(len(y_data_init)):\n",
        "            encoded = []\n",
        "            for c in y_data_init[i]:\n",
        "                if c=='':\n",
        "                  # changer map char et ajouter en haut\n",
        "                    encoded.append(self.char_dict['<SPACE>'])\n",
        "                else:\n",
        "                  encoded.append(self.char_dict[c])\n",
        "\n",
        "            y_len = len(encoded)\n",
        "\n",
        "\n",
        "            for j in range(len(encoded), y_max_length):\n",
        "                    encoded.append(float(255))\n",
        "            y_data.append(encoded)\n",
        "            y_data_len.append(y_len)\n",
        "\n",
        "        # convert to array\n",
        "        y_data = np.array(y_data)\n",
        "        label_length = np.array(y_data_len)\n",
        "\n",
        "        return y_data, label_length\n",
        "\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "       # Generate data\n",
        "        x_data_init, y_data_init = self.get_data(list_IDs_temp)\n",
        "\n",
        "        #compute the spec frame of the longest audio\n",
        "        x_data, input_length = self.extract_features(x_data_init)\n",
        "        y_data, label_length = self.encode_text(y_data_init)\n",
        "\n",
        "        inputs = [x_data, y_data, input_length, label_length]\n",
        "        outputs = np.zeros([self.batch_size])\n",
        "        return inputs,outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLskg_tCBdyh"
      },
      "source": [
        "def update_dataframe(csv_path):\n",
        "    ## enleve les colonnes dont on n'a pas besoin\n",
        "    data = pd.read_csv(csv_path)\n",
        "    data = data.dropna(subset=['filename'])\n",
        "    data = data.drop(['test_or_train', 'dialect_region', 'filename',\n",
        "                      'path_from_data_dir', 'is_audio', 'is_word_file',\n",
        "                      'is_phonetic_file', 'is_sentence_file',\n",
        "                      'speaker_id', 'index'], axis=1)\n",
        "\n",
        "    data = data[(data.is_converted_audio == True)]\n",
        "    data = data.reset_index(drop=True)\n",
        "    paths = list(data['path_from_data_dir_windows'])\n",
        "    return data,paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBgsKFiRCBwO",
        "outputId": "65815e0f-4bd6-4ae5-b433-764f658f1bdf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4nUBmPTtlAT"
      },
      "source": [
        "# !cp -r /content/drive/MyDrive/Speech_recognition_dataset  /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn3NmR9DEYCq"
      },
      "source": [
        "#\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "train_csv = '/content/drive/MyDrive/Speech_recognition_dataset/train_data.csv' #path to train.csv\n",
        "path = '/content/drive/MyDrive/Speech_recognition_dataset/data/' #path to TIMIT data\n",
        "\n",
        "\n",
        "train_dataframe,train_path = update_dataframe(train_csv)\n",
        "\n",
        "train_path = [path + x.replace('\\\\', '/') for x in train_path]\n",
        "valid_path=train_path[4160:-12]\n",
        "train_path=train_path[0:4160]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvWlXFxdLpA5",
        "outputId": "cfcb91e9-21b4-4674-d600-a8a177fbdb11"
      },
      "source": [
        "print(len(train_path))\n",
        "print(len(valid_path))\n",
        "data_params = {'window_len' : 128,\n",
        "               'hop_len' : 127,\n",
        "               'nfft' : 256,\n",
        "               'batch_size' : 32,\n",
        "               'shuffle' : True\n",
        "               }\n",
        "\n",
        "print(\"Data Generation\")\n",
        "training_generator = DataGenerator(train_path, **data_params)\n",
        "validation_generator = DataGenerator(valid_path, **data_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4160\n",
            "448\n",
            "Data Generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVCrv_cy1Tml"
      },
      "source": [
        "# !pip install keras_ctcmodel\n",
        "# x_data_init, y_data_init = training_generator.get_data(train_path[0:32])\n",
        "# x_data, input_length = training_generator.extract_features(x_data_init)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIICq6uvG7qz"
      },
      "source": [
        "# print(input_length)\n",
        "# print(x_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJUcSMq7Brl4"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5fEk9DxBjox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e638cb-553e-4c72-88f4-e2e52858884d"
      },
      "source": [
        "from tensorflow.keras.layers import Masking\n",
        "\n",
        "units=500\n",
        "nb_features=129\n",
        "optimizer = Adam(lr=0.0001)\n",
        "nb_labels=28\n",
        "\n",
        "x_input = Input((None,nb_features))\n",
        "x = Masking(mask_value=0)(x_input)\n",
        "print(x.shape)\n",
        "x = Bidirectional(LSTM(units,return_sequences=True))(x)\n",
        "print(x.shape)\n",
        "x = Bidirectional(LSTM(units,return_sequences=True))(x)\n",
        "print(x.shape)\n",
        "x = Bidirectional(LSTM(units, return_sequences=True))(x)\n",
        "print(x.shape)\n",
        "x = Bidirectional(LSTM(units,return_sequences=True))(x)\n",
        "print(x.shape)\n",
        "x = Bidirectional(LSTM(units,return_sequences=True))(x)\n",
        "print(x.shape)\n",
        "y_pred = TimeDistributed(Dense(units=nb_labels, activation='softmax'), name='softmax')(x)\n",
        "print(y_pred.shape)\n",
        "# x_1 = tf.keras.Model(inputs=x_input, outputs= y_pred)\n",
        "# x_1.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 129)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 1000)\n",
            "(None, None, 1000)\n",
            "(None, None, 1000)\n",
            "(None, None, 1000)\n",
            "(None, None, 1000)\n",
            "(None, None, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6YjMLxZksP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382ade55-739d-429e-be83-a7f835d6a700"
      },
      "source": [
        "# CTC\n",
        "\n",
        "model =CTCModel ([x_input], [y_pred])\n",
        "model.compile(optimizer=optimizer)\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 129)]  0           []                               \n",
            "                                                                                                  \n",
            " masking (Masking)              (None, None, 129)    0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, None, 1000)   2520000     ['masking[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, None, 1000)  6004000     ['bidirectional[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirectional  (None, None, 1000)  6004000     ['bidirectional_1[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_3 (Bidirectional  (None, None, 1000)  6004000     ['bidirectional_2[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_4 (Bidirectional  (None, None, 1000)  6004000     ['bidirectional_3[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " softmax (TimeDistributed)      (None, None, 28)     28028       ['bidirectional_4[0][0]']        \n",
            "                                                                                                  \n",
            " labels (InputLayer)            [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_length (InputLayer)      [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " label_length (InputLayer)      [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " CTCloss (Lambda)               (None, 1)            0           ['softmax[0][0]',                \n",
            "                                                                  'labels[0][0]',                 \n",
            "                                                                  'input_length[0][0]',           \n",
            "                                                                  'label_length[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 26,564,028\n",
            "Trainable params: 26,564,028\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "xiVe5_9W2zrE",
        "outputId": "1cb63e0d-2dcb-4504-a0f7-f3ba569a46de"
      },
      "source": [
        "history=model.fit(training_generator,epochs=3,validation_data=validation_generator)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "  1/130 [..............................] - ETA: 2:17:51 - loss: 1063.3660"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-131cca273438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/CTCModel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    353\u001b[0m                                    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                                    \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                                    steps_per_epoch=steps_per_epoch, validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUz8Am1K2Ach"
      },
      "source": [
        "# # model.save('/content/drive/MyDrive/Speech_recognition_dataset/model_3')\n",
        "# import tensorflow as tf\n",
        "# model.save(\"my_h5_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeEv_4Rs2Pf5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "\n",
        "# summarize\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRtcgIhVhGe-"
      },
      "source": [
        "test_csv = '/content/drive/MyDrive/Speech_recognition_dataset/test_data.csv' #path to test.csv\n",
        "path = '/content/drive/MyDrive/Speech_recognition_dataset/data/' #path to TIMIT data\n",
        "\n",
        "\n",
        "test_dataframe,test_path = update_dataframe(test_csv)\n",
        "\n",
        "test_path = [path + x.replace('\\\\', '/') for x in test_path]\n",
        "test_path=test_path[0:-16]\n",
        "\n",
        "test_generator = DataGenerator(test_path, **data_params)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddWuIxo85b8_"
      },
      "source": [
        "\n",
        "x_data_init, y_data_init = test_generator.get_data(test_path)\n",
        "\n",
        "#compute the spec frame of the longest audio\n",
        "x_data, input_length = test_generator.extract_features(x_data_init)\n",
        "y_data, label_length = test_generator.encode_text(y_data_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iuj28Tjki6tY"
      },
      "source": [
        "\n",
        "print(\"Evaluation\")\n",
        "model.save_model('/content/drive/MyDrive/Speech_recognition_dataset')\n",
        "from CTCModel import CTCModel as CTCModel\n",
        "model.evaluate([x_data, y_data, input_length, label_length],batch_size=32,metrics=['ler', 'ser'])\n",
        "\n",
        "\n",
        "\n",
        "# for i in range(10):  # print the 10 first predictions\n",
        "#     print(\"Prediction :\", [j for j in pred[i] if j!=-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wUqBDtKSQCw"
      },
      "source": [
        "from CTCModel import CTCModel as CTCModel\n",
        "print(\"Prediction\")\n",
        "pred = model.predict([x_data, input_length],steps=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKYUwjjoFfl0"
      },
      "source": [
        "def decode(sequence):\n",
        "    unpaded = [j for j in sequence if j != -1]\n",
        "    pred = []\n",
        "    for c in unpaded:\n",
        "        if c == 0:\n",
        "            pred.append(\" \")\n",
        "        else:\n",
        "            pred.append(char_dict[c])\n",
        "    pred = ''.join(pred)\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}