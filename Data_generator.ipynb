{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IceOnDunes/speech-recognition/blob/main/Data_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTX_otxg2Pyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a04178-e5ef-4ef6-ba29-06468f431286"
      },
      "source": [
        "\"\"\" Classe Data generator pour génerer les données par batch durant l'apprentissage \"on the fly\" \n",
        "    code de classe inspiré de : https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
        "    Ajout de la partie pre-traitement (normalisation, calcul de spectrogramme, padding ...) \n",
        "\"\"\"\n",
        "!pip install keras --upgrade\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs,shuffle=False, batch_size=32, window_len=128,nfft=254,hop_len=127):\n",
        "        'Initialization'\n",
        "        self.list_IDs = list_IDs\n",
        "        self.hop_len = hop_len\n",
        "        self.batch_size = batch_size\n",
        "        self.nfft= nfft\n",
        "        self.window_len = window_len\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.char_dict =  { 'a': 1,\n",
        "                            'b' : 2,\n",
        "                            'c' : 3,\n",
        "                            'd' : 4,\n",
        "                            'e' : 5,\n",
        "                            'f' : 6,\n",
        "                            'g' : 7,\n",
        "                            'h' : 8,\n",
        "                            'i' : 9,\n",
        "                            'j' : 10,\n",
        "                            'k' : 11,\n",
        "                            'l' : 12,\n",
        "                            'm' : 13,\n",
        "                            'n' : 14,\n",
        "                            'o' : 15,\n",
        "                            'p' : 16,\n",
        "                            'q' : 17,\n",
        "                            'r' : 18,\n",
        "                            's' : 19,\n",
        "                            't' : 20,\n",
        "                            'u' : 21,\n",
        "                            'v' : 22,\n",
        "                            'w' : 23,\n",
        "                            'x' : 24,\n",
        "                            'y' : 25,\n",
        "                            'z' : 26,\n",
        "                            ' ': 27}\n",
        "                          \n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def get_data (data_IDs):\n",
        "      \"\"\"renvoie le son ainsi que son text associé normalisé \"\"\"\n",
        "\n",
        "      son = []\n",
        "      text = []\n",
        "\n",
        "      \"récuperer les characters de ponctuations\"\n",
        "      ponctuation_char = set(string.punctuation)\n",
        "\n",
        "      for d in data_IDs:\n",
        "          fe, audio = wavfile.read(d)\n",
        "          moyenne = audio.mean()\n",
        "          std = audio.std()\n",
        "          audio = (audio-moyenne) / std\n",
        "          son.append(audio)\n",
        "\n",
        "          text_path = d[:-8] + '.TXT'  # conversion chemin audio vers chemin text correspondant\n",
        "          txt = open( text_path, 'r')\n",
        "          ligne = txt.readline()\n",
        "\n",
        "\n",
        "          # normaliser les texts\n",
        "          result = ''.join([i for i in ligne if (not i.isdigit() and i not in ponctuation_char )]).lstrip().lower().replace(\"\\n\", '')\n",
        "          text.append(result)\n",
        "      return son, text\n",
        "\n",
        "\n",
        "      def get_padded_mel_spectro(audio, maxlen, n_fft=self.nfft, hop_length=self.hop_len, win_length=self.window_len):\n",
        "\n",
        "          mel_spectro =librosa.feature.melspectrogram(audio.astype(float), n_fft ,hop_length, win_length)\n",
        "          x_len = spec.shape[1]\n",
        "\n",
        "          padded_spectro = pad_sequences(mel_spectro,maxlen=maxlen, dtype='float', padding='post', truncating='post')\n",
        "          return padded_spectro, x_len\n",
        "\n",
        "    def extract_features(self, x_data_init):\n",
        "      #extract longest spectrogram length\n",
        "        mel_spectrogram = librosa.feature.melspectrogram(max(x_data_init, key=len).astype(float), n_fft= self.nfft,hop_length=self.hop_len, win_length=self.window_len)\n",
        "        x_max_length = mel_spectrogram.shape[1]\n",
        "\n",
        "        x_data = []\n",
        "        x_data_len = []\n",
        "\n",
        "        for i in range(len(x_data_init)):\n",
        "\n",
        "            padded_spectro, x_len = get_padded_mel_spectro(x_data_init[i], maximum_len=x_max_length , n_fft= self.nfft,hop_length=self.hop_len, win_length=self.window_len)\n",
        "\n",
        "            x_data.append(padded_spectro)\n",
        "            x_data_len.append(x_len) \n",
        "\n",
        "        #convert to array\n",
        "        data_input = np.array(x_data)\n",
        "        input_length = np.array(x_data_len)\n",
        "\n",
        "        return data_input, input_length\n",
        "\n",
        "#changer les noms\n",
        "    def encode_text(self, y_data_init):\n",
        "\n",
        "        y_max_length = len(max(y_data_init, key=len))\n",
        "        y_data = []\n",
        "        y_data_len = []\n",
        "\n",
        "        for i in range(len(y_data_init)):\n",
        "            encoded = []\n",
        "            for c in y_data_init[i]:\n",
        "                if c=='':\n",
        "                  # changer map char et ajouter en haut\n",
        "                    encoded.append(self.char_dict['<SPACE>'])\n",
        "                else:\n",
        "                    encoded.append(self.char_dict[c])\n",
        "\n",
        "            y_len = len(encoded)\n",
        "\n",
        "\n",
        "            for j in range(len(encoded), y_max_length):\n",
        "                    encoded.append(float(0))\n",
        "            y_data.append(y)\n",
        "            y_data_len.append(y_len)\n",
        "\n",
        "        # convert to array\n",
        "        y_data = np.array(y_data)\n",
        "        label_length = np.array(y_data_len)\n",
        "\n",
        "        return y_data, label_length\n",
        "\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "       # Generate data\n",
        "        x_data_init, y_data_init = get_data(list_IDs_temp)\n",
        "\n",
        "        #compute the spec frame of the longest audio\n",
        "        x_data, input_length = self.extract_features(x_data_init)\n",
        "        y_data, label_length = self.encode_text(y_data_init)\n",
        "\n",
        "        inputs = [x_data, y_data, input_length, label_length]\n",
        "        outputs = np.zeros([self.batch_size])\n",
        "        return inputs, outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLskg_tCBdyh"
      },
      "source": [
        "def update_dataframe(csv_path):\n",
        "    ## enleve les colonnes dont on n'a pas besoin\n",
        "    data = pd.read_csv(csv_path)\n",
        "    data = data.dropna(subset=['filename'])\n",
        "    data = data.drop(['test_or_train', 'dialect_region', 'filename',\n",
        "                      'path_from_data_dir', 'is_audio', 'is_word_file',\n",
        "                      'is_phonetic_file', 'is_sentence_file',\n",
        "                      'speaker_id', 'index'], axis=1)\n",
        "\n",
        "    data = data[(data.is_converted_audio == True)]\n",
        "    data = data.reset_index(drop=True)\n",
        "    paths = list(data['path_from_data_dir_windows'])\n",
        "    return data,paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn3NmR9DEYCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6afe4436-4e2f-4743-dc7a-cb158094bd36"
      },
      "source": [
        "#To change with your paths\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "train_csv = '/content/drive/MyDrive/Speech_recognition_dataset/train_data.csv' #path to train.csv\n",
        "path = '/content/drive/MyDrive/Speech_recognition_dataset/data/' #path to TIMIT data\n",
        "\n",
        "###Train Data preparation\n",
        "\n",
        "#Extract and clean dataframe from cvs\n",
        "train_dataframe,train_path = update_dataframe(train_csv)\n",
        "\n",
        "#Extract test paths from data frame :\n",
        "train_path = [path + x.replace('\\\\', '/') for x in train_path]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvWlXFxdLpA5",
        "outputId": "dd4ec70a-db65-49ee-a3c8-336c12663cbc"
      },
      "source": [
        "print(len(train_path))\n",
        "data_params = {'window_len' : 128,\n",
        "               'hop_len' : 127,\n",
        "               'nfft' : 256,\n",
        "               'batch_size' : 32,\n",
        "               'shuffle' : True\n",
        "               }\n",
        "\n",
        "print(\"Data Generation\")\n",
        "training_generator = DataGenerator(train_path, **data_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4620\n",
            "Data Generation\n"
          ]
        }
      ]
    }
  ]
}