{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6hX918kaDn_"
      },
      "source": [
        "from CTCModel import CTCModel as CTCModel\n",
        "from google.colab import drive\n",
        "from Model import model\n",
        "from fcts import update_dataframe,decode\n",
        "import matplotlib.pyplot as plt\n",
        "from Data_generator import DataGenerator\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.optimizers  import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJSu2wwYaPsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7e4de4-41c6-4cf2-dc9e-c89ed36bd142"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "test_csv = '/content/drive/MyDrive/Speech_recognition_dataset/test_data.csv' #path to test.csv\n",
        "test_dataframe,test_path = update_dataframe(test_csv)\n",
        "path = '/content/drive/MyDrive/Speech_recognition_dataset/data/' #path to TIMIT data\n",
        "test_path = [path + x.replace('\\\\', '/') for x in test_path]\n",
        "test_path=test_path[0:-16]\n",
        "\n",
        "\"\"\"\n",
        "Data Generators\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "data_params = {'window_len' : 128,\n",
        "               'hop_len' : 127,\n",
        "               'nfft' : 256,\n",
        "               'batch_size' : 32,\n",
        "               'shuffle' : True\n",
        "               }\n",
        "test_generator = DataGenerator(test_path, **data_params)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ1F3KQqaTnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08a3159-b346-4c7e-f716-04fc467d6a26"
      },
      "source": [
        "\"\"\"\n",
        "Model training\n",
        "\n",
        "\"\"\"\n",
        "model = model()\n",
        "model.compile(optimizer=Adam(lr=0.0001))\n",
        "model.load_weights('/content/drive/MyDrive/Speech_recognition_dataset/Models/saved-model_6-19-60.64.hdf5')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 129)\n",
            "(None, None, 1000)\n",
            "(None, None, 1000)\n",
            "(None, None, 1000)\n",
            "(None, None, 1000)\n",
            "(None, None, 1000)\n",
            "(None, None, 28)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Evaluation\n",
        "\"\"\"\n",
        "evaluation = model.evaluate_generator(test_generator,metrics=['ler','ser'])\n",
        "evaluation = np.array(evaluation)\n",
        "print(\"Label Error Rate = \" , np.mean(evaluation[0])*100,\"%\")\n",
        "print(\"Sequence Error Rate = \" , evaluation[1]*100,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I2NZr3QrUyy",
        "outputId": "dc8a6a2d-7fb4-435b-cc73-062def6143c8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/CTCModel.py:515: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  verbose=verbose)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Error Rate =  35.43839156627655 %\n",
            "Sequence Error Rate =  95.25240384615384 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdMVxTM8aYoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce674012-a9bf-4d4c-b908-3b9902e69026"
      },
      "source": [
        "\"\"\"\n",
        "Prediction\n",
        "\"\"\"\n",
        "print(\"Prediction\")\n",
        "pred = model.predict_generator(test_generator,steps=52)\n",
        "\"\"\"\n",
        "Decoding prediction results\n",
        "\"\"\"\n",
        "y_pred = []\n",
        "y_true = []\n",
        "for i in range (len(pred[0])):\n",
        "  y_pred.append(decode(pred[0][i]))\n",
        "  y_true.append(decode(pred[1][i]))\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from jiwer import wer\n",
        "WER_error = wer(y_true, y_pred)\n",
        "print(\"Word Error Rate = \",WER_error*100,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAM0DQ-OxKvD",
        "outputId": "68b59c0c-c6ca-487a-dbad-fdd287246293"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Error Rate =  73.38204592901879 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaK8YVorafYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c09d5e-6f96-46b0-eeb0-977691585695"
      },
      "source": [
        "\"\"\"\n",
        "Predictions Visualisation\n",
        "\"\"\"\n",
        "\n",
        "for i in range (10):\n",
        "  print(\"(\",i+1,\")\")\n",
        "  y_pred=decode(pred[0][i])\n",
        "  print(\"predicted : \",y_pred)\n",
        "\n",
        "  y_true=decode(pred[1][i])\n",
        "  \n",
        "  print(\"Ground truth : \",y_true)\n",
        "  print('\\n')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "( 1 )\n",
            "predicted :  dontt  ask  me  to  carry  an  oily  rag  like  that\n",
            "Ground truth :  dont  ask  me  to  carry  an  oily  rag  like  that\n",
            "\n",
            "\n",
            "( 2 )\n",
            "predicted :  ha  twilyd  of  the  tie  t  de  wil  hae  shibly\n",
            "Ground truth :  at  twilight  on  the  twelfth  day  well  have  chablis\n",
            "\n",
            "\n",
            "( 3 )\n",
            "predicted :  abig  go  iblye  amble  throi  the  far  near\n",
            "Ground truth :  a  big  goat  idly  ambled  through  the  farmyard\n",
            "\n",
            "\n",
            "( 4 )\n",
            "predicted :  gu  ahacco  abionthies  eur  hep\n",
            "Ground truth :  got  a  heck  of  a  buy  on  this  dirt  cheap\n",
            "\n",
            "\n",
            "( 5 )\n",
            "predicted :  seas  thanter  then    aon\n",
            "Ground truth :  she  is  thinner  than  i  am\n",
            "\n",
            "\n",
            "( 6 )\n",
            "predicted :  en  spivi  nitly  in  crease  strangh  brae  isly\n",
            "Ground truth :  eating  spinach  nightly  increases  strength  miraculously\n",
            "\n",
            "\n",
            "( 7 )\n",
            "predicted :  she  had  your  dark  suit  in  greasy  wash  water  al  year\n",
            "Ground truth :  she  had  your  dark  suit  in  greasy  wash  water  all  year\n",
            "\n",
            "\n",
            "( 8 )\n",
            "predicted :  wea  ing  ut  i  fou  diid  he  hapatiod  y\n",
            "Ground truth :  we  can  get  it  if  we  dig  he  said  patiently\n",
            "\n",
            "\n",
            "( 9 )\n",
            "predicted :  anoovf  aucamnnti  his  vorye\n",
            "Ground truth :  a  note  of  awe  came  into  his  voice\n",
            "\n",
            "\n",
            "( 10 )\n",
            "predicted :  de  vitthaufante  al  ouh\n",
            "Ground truth :  do  without  fancy  tablecloths\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}